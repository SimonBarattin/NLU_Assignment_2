{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install spacy-conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import conll\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_doc(path):\n",
    "    file = conll.read_corpus_conll(path, ' ')\n",
    "    for tst in file:\n",
    "        if tst[0][0] == '-DOCSTART-':\n",
    "            file.remove(tst)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that allows us to change the spacy representation, so that we can actually compare the results with the conll ones\n",
    "def changeRep(token):\n",
    "    if token.ent_iob_ != 'O':\n",
    "        if token.ent_type_ == 'PERSON':\n",
    "            return token.ent_iob_ + '-PER'\n",
    "        if token.ent_type_ == 'ORG':\n",
    "            return token.ent_iob_ + '-ORG'\n",
    "        if (token.ent_type_ == 'GPE') or (token.ent_type_ == 'LOC') or (token.ent_type_ == 'FAC'):\n",
    "            return token.ent_iob_ + '-LOC'\n",
    "        if token.ent_type == '':\n",
    "                return 'O'\n",
    "        else:\n",
    "            return token.ent_iob_ + '-MISC'\n",
    "    else:\n",
    "        return token.ent_iob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(refs, hyps):\n",
    "    pred = []\n",
    "    true = []\n",
    "    for ent in refs:\n",
    "        true.append(ent[0][1])\n",
    "    for ent in hyps:\n",
    "        pred.append(ent[0][1])\n",
    "    res = classification_report(true, pred)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent(sent):\n",
    "    res = []\n",
    "    token = ''\n",
    "    for t in sent:\n",
    "        if t.whitespace_:\n",
    "            token += t.text\n",
    "            label = changeRep(t)\n",
    "            res.append((token, label))\n",
    "            token = ''\n",
    "        else:\n",
    "            token += t.text\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain the data from conll2003 dataset and remove the -DOCSTART- entries\n",
    "def evaluate(path):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    file = remove_doc(path)\n",
    "    refs = [[(text, iob) for text, pos, _, iob in sent] for sent in file]\n",
    "    hyps = []\n",
    "    for row in file:\n",
    "        text = ''.join(t[0]+' ' for t in row)\n",
    "        doc = nlp(text)\n",
    "        tokenized = get_sent(doc)\n",
    "        hyps.append(tokenized)\n",
    "\n",
    "    accuracies = get_stats(refs, hyps)\n",
    "    print(accuracies)\n",
    "\n",
    "    #run the conll evaluation function to obtain the chunk level accuracies\n",
    "    results = conll.evaluate(refs, hyps)\n",
    "\n",
    "    pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "    print('\\t1.2 - report CoNLL chunk-level performance (per class and total)\\n')\n",
    "    print(pd_tbl.round(decimals=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_ents(sent):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(sent)\n",
    "    groups = []\n",
    "    ne = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        temp = []\n",
    "        for e in chunk.ents:\n",
    "            if e not in ne:\n",
    "                ne.append(e)\n",
    "                temp.append(e.label_)\n",
    "        if ne and len(temp) != 0:\n",
    "            groups.append(temp)\n",
    "    for ent in doc.ents:\n",
    "        if ent not in ne:\n",
    "            groups.append([ent.label_])        \n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequencies(path):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    file = remove_doc(path)\n",
    "    groups = {}\n",
    "    for row in file[:100]:\n",
    "        sent = ''.join(t[0]+' ' for t in row)\n",
    "        g = group_ents(sent)\n",
    "        for group in g:\n",
    "            st = ''\n",
    "            for x in group:\n",
    "                st += x + ' '\n",
    "            if st not in groups:\n",
    "                groups[st] = 1\n",
    "            else:\n",
    "                groups[st] += 1\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_head(dep, token):\n",
    "    if token.dep_ == 'compound':\n",
    "        dep.append(token)\n",
    "        get_head(dep, token.head)\n",
    "    else:\n",
    "        dep.append(token)\n",
    "    return dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compound_seg(sent):\n",
    "    res = []\n",
    "    compounds = []\n",
    "    skip = []\n",
    "    ent_c = []\n",
    "    indexes = []\n",
    "    for t in sent:\n",
    "        if t.dep_ == 'compound':\n",
    "            compounds.append(get_head([], t))\n",
    "    for span in compounds:\n",
    "        indexes.append([t.i for t in span])\n",
    "        for t in span:            \n",
    "            skip.append(t.i)\n",
    "    for t in sent:\n",
    "        if (not t.i in skip):\n",
    "            indexes.append([t.i])\n",
    "    indexes = sorted(indexes, key=lambda x: x[0])\n",
    "    for ind in indexes:\n",
    "        if len(ind) == 1:\n",
    "            ent_c.append(sent[ind[0]:ind[0]+1])\n",
    "        else:\n",
    "            ent_c.append(sent[ind[0]:ind[-1]+1])\n",
    "    return ent_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats_comp(refs, hyps):\n",
    "    pred = []\n",
    "    true = []\n",
    "    for ent in refs:\n",
    "        true.append(ent[0][1])\n",
    "    for ent in hyps:\n",
    "        pred.append(ent[1])\n",
    "    res = classification_report(true, pred)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('SOCCER', 'O'), ('-', 'O'), ('JAPAN', 'O'), ('GET', 'O'), ('LUCKY', 'O'), ('WIN', 'O'), ('-', 'O'), ('JAPAN', 'O'), ('GET', 'O'), ('LUCKY', 'O'), ('WIN', 'O'), ('GET', 'O'), ('LUCKY', 'O'), ('WIN', 'O'), ('LUCKY', 'O'), ('WIN', 'O'), (',', 'O'), ('CHINA', 'B-LOC'), ('IN', 'O'), ('SURPRISE', 'O'), ('DEFEAT', 'O'), ('.', 'O'), ('Nadim', 'B-ORG'), ('Ladki', 'I-ORG'), ('AL', 'B-ORG'), ('-', 'I-ORG'), ('AIN', 'I-ORG'), ('-', 'I-ORG'), (',', 'O'), ('United', 'B-ORG'), ('Arab', 'I-ORG'), ('Emirates', 'I-ORG'), ('Arab', 'I-ORG'), ('Emirates', 'I-ORG'), ('1996', 'B-MISC'), ('-', 'I-MISC'), ('12', 'I-MISC'), ('-', 'I-MISC'), ('06', 'I-MISC'), ('Japan', 'B-LOC'), ('began', 'O'), ('the', 'O'), ('defence', 'O'), ('of', 'O'), ('their', 'B-MISC'), ('Asian', 'I-MISC'), ('Cup', 'I-MISC'), ('title', 'O'), ('Cup', 'I-MISC'), ('title', 'O'), ('with', 'O'), ('a', 'O'), ('lucky', 'O'), ('2', 'O'), ('-', 'O'), ('1', 'O'), ('win', 'O'), ('against', 'O'), ('Syria', 'B-LOC'), ('in', 'O'), ('a', 'O'), ('Group', 'B-ORG'), ('C', 'I-ORG'), ('championship', 'O'), ('match', 'O'), ('C', 'I-ORG'), ('championship', 'O'), ('match', 'O'), ('championship', 'O'), ('match', 'O'), ('on', 'O'), ('Friday', 'B-MISC'), ('.', 'O'), ('But', 'O'), ('China', 'B-LOC'), ('saw', 'O'), ('their', 'O'), ('luck', 'O'), ('desert', 'O'), ('them', 'O'), ('in', 'O'), ('the', 'O'), ('second', 'B-MISC'), ('match', 'O'), ('of', 'O'), ('the', 'O'), ('group', 'O'), (',', 'O'), ('crashing', 'O'), ('to', 'O'), ('a', 'O'), ('surprise', 'O'), ('2', 'B-MISC'), ('-', 'O'), ('0', 'O'), ('defeat', 'O'), ('to', 'O'), ('newcomers', 'O'), ('Uzbekistan', 'B-LOC'), ('.', 'O')]\n",
      "[('But', 'O'), ('China', 'B-LOC'), ('saw', 'O'), ('their', 'O'), ('luck', 'O'), ('desert', 'O'), ('them', 'O'), ('in', 'O'), ('the', 'O'), ('second', 'O'), ('match', 'O'), ('of', 'O'), ('the', 'O'), ('group', 'O'), (',', 'O'), ('crashing', 'O'), ('to', 'O'), ('a', 'O'), ('surprise', 'O'), ('2-0', 'O'), ('defeat', 'O'), ('to', 'O'), ('newcomers', 'O'), ('Uzbekistan', 'B-LOC'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "def seg_err(path):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    file = remove_doc(path)\n",
    "    hyps = []\n",
    "    refs = [[(text, iob) for text, pos, _, iob in sent] for sent in file[:5]]\n",
    "    for row in file[:5]:\n",
    "        text = ''.join(t[0]+' ' for t in row)\n",
    "        doc = nlp(text)\n",
    "        ents = compound_seg(doc)\n",
    "        for e in ents:\n",
    "            for t in e:\n",
    "                hyps.append((t.text, changeRep(t)))\n",
    "                #refs.append((t.text, (f'{t.ent_iob_}-{t.ent_type_}' if t.ent_iob_ != 'O' else t.ent_iob_)))\n",
    "        \n",
    "    #accuracies = get_stats_comp(refs, hyps)\n",
    "    #print(accuracies)\n",
    "\n",
    "    #run the conll evaluation function to obtain the chunk level accuracies\n",
    "    #results = conll.evaluate(refs, hyps)\n",
    "\n",
    "    #pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "    #print('\\t1.2 - report CoNLL chunk-level performance (per class and total)\\n')\n",
    "    #print(pd_tbl.round(decimals=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------#1-----------------------------\n",
      "\n",
      "Evaluate spaCy NER on CoNLL 2003 data (provided)\n",
      "\t1.1 - report token-level performance (per class and total)\n",
      "\n",
      "275   257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\miniconda3\\envs\\NLU\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\miniconda3\\envs\\NLU\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\miniconda3\\envs\\NLU\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.47      0.60      0.52       317\n",
      "      B-MISC       0.58      0.27      0.37       248\n",
      "       B-ORG       0.22      0.53      0.31       306\n",
      "       B-PER       0.55      0.62      0.58       266\n",
      "      I-MISC       0.00      0.00      0.00        11\n",
      "       I-ORG       0.00      0.00      0.00        11\n",
      "       I-PER       0.00      0.00      0.00         2\n",
      "           O       0.87      0.71      0.78      2292\n",
      "\n",
      "    accuracy                           0.64      3453\n",
      "   macro avg       0.34      0.34      0.32      3453\n",
      "weighted avg       0.72      0.64      0.67      3453\n",
      "\n",
      "\t1.2 - report CoNLL chunk-level performance (per class and total)\n",
      "\n",
      "           p      r      f     s\n",
      "LOC    0.749  0.671  0.708  1668\n",
      "PER    0.774  0.609  0.681  1617\n",
      "MISC   0.111  0.546  0.184   702\n",
      "ORG    0.464  0.276  0.346  1661\n",
      "total  0.408  0.521  0.458  5648\n",
      "\n",
      "-----------------------------#2-----------------------------\n",
      "\n",
      "Test sentence: Apple's Steve Jobs died in 2011 in Palo Alto, California.\n",
      " Result: [['ORG', 'PERSON'], ['GPE'], ['GPE'], ['DATE']]\n",
      "Frequencies:\n",
      "\n",
      "CARDINAL  :  67\n",
      "GPE  :  65\n",
      "PERSON  :  62\n",
      "DATE  :  22\n",
      "ORG  :  21\n",
      "TIME  :  15\n",
      "CARDINAL PERSON  :  10\n",
      "NORP  :  8\n",
      "ORDINAL  :  7\n",
      "EVENT  :  6\n",
      "GPE GPE  :  2\n",
      "NORP PERSON  :  1\n",
      "GPE PERSON  :  1\n",
      "DATE EVENT  :  1\n",
      "ORDINAL NORP  :  1\n",
      "CARDINAL GPE  :  1\n",
      "GPE PERSON CARDINAL  :  1\n",
      "LAW  :  1\n",
      "\n",
      "-----------------------------#3-----------------------------\n",
      "\n",
      "Fix segmentation errors.\n"
     ]
    }
   ],
   "source": [
    "print('-----------------------------#1-----------------------------\\n')\n",
    "print('Evaluate spaCy NER on CoNLL 2003 data (provided)')\n",
    "print('\\t1.1 - report token-level performance (per class and total)\\n')\n",
    "evaluate('src/conll2003/test.txt')\n",
    "print('\\n-----------------------------#2-----------------------------\\n')\n",
    "tst = \"Apple's Steve Jobs died in 2011 in Palo Alto, California.\"\n",
    "print(\"Test sentence: Apple's Steve Jobs died in 2011 in Palo Alto, California.\\n\", f'Result: {group_ents(tst)}')\n",
    "frequencies = get_frequencies('src/conll2003/test.txt')\n",
    "sort = {k: v for k, v in sorted(frequencies.items(), key=lambda item: item[1], reverse=True)}\n",
    "print('Frequencies:\\n')\n",
    "for y in sort:\n",
    "    print(y,': ',sort[y])\n",
    "print('\\n-----------------------------#3-----------------------------\\n')\n",
    "print('Fix segmentation errors.')\n",
    "seg_err('src/conll2003/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
